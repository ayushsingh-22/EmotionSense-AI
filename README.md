# ğŸ§  Emotion AI - Multi-Modal Emotion Detection Platform

A comprehensive, production-ready emotion analysis platform that combines text, voice, and conversational AI to provide empathetic responses and emotional intelligence insights.

## âœ¨ NEW FEATURES IN THIS UPDATE

### ğŸ” **Authentication & User Management**
- Secure user authentication with Supabase
- User profiles with emotion analytics
- Session management and data persistence

### ğŸ’¬ **AI Chat Companion**
- Conversational emotion analysis
- Empathetic response generation using Gemini API / Groq Llama
- Real-time chat with emotion tracking
- Text-to-speech response playback

### ğŸ¤ **Enhanced Voice Experience** 
- Real-time voice recording with browser MediaRecorder API
- Voice companion mode (like Alexa/Google Assistant)
- Combined text + voice emotion analysis

### ğŸ“Š **Personal Analytics Dashboard**
- Personal emotion journey tracking
- Weekly/monthly emotion trends
- Session statistics and confidence metrics
- Interactive emotion visualizations

### ğŸ¨ **Modern UI/UX**
- Responsive design with TailwindCSS
- Dark/light theme support
- Smooth animations with Framer Motion
- Authentication-aware navigation

## ğŸš€ Quick Start

### Run Everything Together (Recommended)
```bash
# Double-click or run in terminal:
start-all.bat
```
This will open 2 windows:
- Backend on http://localhost:8080
- Frontend on http://localhost:3000

### Run Separately (When Needed)
```bash
# Backend only:
start-backend.bat

# Frontend only (backend must be running):
start-frontend.bat
```

**ğŸ“– See [STARTUP_GUIDE.md](STARTUP_GUIDE.md) for detailed instructions and troubleshooting.**

## ğŸ—„ï¸ Database Setup (NEW)

This update now includes **Supabase integration** for user authentication and data persistence.

### Quick Supabase Setup

1. **Create Account**: Sign up at [supabase.com](https://supabase.com)
2. **Create Project**: Create a new project and note your:
   - Project URL
   - Anon Key
3. **Set up Database**:
   - Go to SQL Editor in Supabase dashboard
   - Run the SQL from `database_schema.sql` (located in project root)
4. **Update Environment**:
   ```bash
   # Frontend (.env.local)
   NEXT_PUBLIC_SUPABASE_URL=your_project_url
   NEXT_PUBLIC_SUPABASE_ANON_KEY=your_anon_key
   ```

### Database Schema Includes:
- **User Profiles**: Extended user information
- **Emotion Sessions**: Analysis history and results  
- **Chat Messages**: Conversational data for chat companion
- **Row Level Security**: Secure data access per user

### Alternative: Local SQLite (Legacy)
You can still use SQLite for local development by configuring the backend environment.



## ğŸ“ Project Structure (Polyrepo)## âœ¨ Key Features



```- ğŸ­ **Dual-Model Text Emotion**: BiLSTM ONNX + HuggingFace DistilRoBERTa

emotion-detection-platform/- ğŸ¤ **Advanced Voice Analysis**: Groq Whisper STT + HuggingFace Wav2Vec2

â”œâ”€â”€ backend/                     # Node.js backend API- ğŸ”„ **Multi-Modal Fusion**: Intelligent weighted combination of text + voice emotions

â”‚   â”œâ”€â”€ src/- ğŸ¤– **Smart LLM Integration**: Google Gemini (primary) with LLaMA fallback via Groq

â”‚   â”‚   â”œâ”€â”€ server.js           # Entry point- ğŸµ **Text-to-Speech**: Optional Piper TTS for voice responses

â”‚   â”‚   â”œâ”€â”€ config/             # Configuration- ğŸ“Š **Dual Database**: Supabase (cloud) or SQLite (local)

â”‚   â”‚   â”œâ”€â”€ routes/             # API routes- âš¡ **ONNX Optimization**: Fast CPU inference with ONNX Runtime

â”‚   â”‚   â”œâ”€â”€ text-service/       # Text emotion analysis- ğŸ” **Production-Ready**: Robust error handling, logging, and fallback mechanisms

â”‚   â”‚   â”œâ”€â”€ voice-service/      # Voice emotion analysis

â”‚   â”‚   â”œâ”€â”€ multi-modal-layer/  # Emotion fusion## ğŸ“Š Emotion Detection Pipeline

â”‚   â”‚   â”œâ”€â”€ llm-service/        # LLM integration

â”‚   â”‚   â”œâ”€â”€ storage-service/    # Data persistence### Text Emotion Analysis (Dual Model)

â”‚   â”‚   â”œâ”€â”€ middleware/         # Express middleware```

â”‚   â”‚   â””â”€â”€ utils/              # UtilitiesText Input

â”‚   â”œâ”€â”€ models/                 # ML models (ONNX)    â†“

â”‚   â”œâ”€â”€ data/                   # SQLite databaseBiLSTM ONNX Model (6 emotions) + HuggingFace DistilRoBERTa (7 emotions)

â”‚   â”œâ”€â”€ logs/                   # Application logs    â†“ (parallel execution)

â”‚   â”œâ”€â”€ temp/                   # Temporary filesWeighted Combination (50/50)

â”‚   â”œâ”€â”€ package.json    â†“

â”‚   â””â”€â”€ README.md              # Backend documentationCombined Emotion Result

â”‚```

â”œâ”€â”€ frontend/                   # Frontend application (React/Vue/Next.js)

â”‚   â”œâ”€â”€ src/### Voice Emotion Analysis (Multi-Stage)

â”‚   â”‚   â”œâ”€â”€ components/```

â”‚   â”‚   â”œâ”€â”€ pages/Audio Input

â”‚   â”‚   â”œâ”€â”€ services/    â†“

â”‚   â”‚   â””â”€â”€ utils/Groq Whisper API (Speech-to-Text)

â”‚   â”œâ”€â”€ package.json    â†“

â”‚   â””â”€â”€ README.md              # Frontend documentationTranscription

â”‚    â†“

â”œâ”€â”€ .git/                       # Git repositoryâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”œâ”€â”€ .gitignore                  # Root gitignoreâ”‚   Text Emotion          â”‚   Voice Emotion          â”‚

â””â”€â”€ README.md                   # This fileâ”‚   (from transcript)     â”‚   (from audio features)  â”‚

```â”‚   BiLSTM + HuggingFace  â”‚   HuggingFace Wav2Vec2   â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## âœ¨ Key Features    â†“

Weighted Fusion

- ğŸ­ **Dual-Model Text Emotion**: BiLSTM ONNX + HuggingFace DistilRoBERTa    â†“

- ğŸ¤ **Advanced Voice Analysis**: Groq Whisper STT + HuggingFace Wav2Vec2Final Combined Emotion

- ğŸ”„ **Multi-Modal Fusion**: Intelligent weighted combination of text + voice emotions```

- ğŸ¤– **Smart LLM Integration**: Google Gemini (primary) with LLaMA fallback via Groq

- ğŸµ **Text-to-Speech**: Optional Piper TTS for voice responses## ğŸš€ Quick Start

- ğŸ“Š **Dual Database**: Supabase (cloud) or SQLite (local)

- âš¡ **ONNX Optimization**: Fast CPU inference with ONNX Runtime### Prerequisites

- ğŸ” **Production-Ready**: Robust error handling, logging, and fallback mechanisms

- **Node.js** 18+ 

## ğŸš€ Quick Start- **Python** 3.8+

- **API Keys**: Groq, Gemini, HuggingFace

### Prerequisites

### Installation

- **Node.js** 18+

- **Python** 3.8+```bash

- **npm** or **yarn**# 1. Clone and install

- **API Keys**: Groq, Gemini, HuggingFacegit clone <repository-url>

cd emotion-detection-backend

### 1. Setup Backendnpm install



```bash# 2. Install Python dependencies

# Navigate to backendpip install onnxruntime numpy

cd backend

# 3. Configure environment

# Install dependenciescp .env.example .env

npm install# Edit .env with your API keys



# Install Python dependencies# 4. Verify ONNX model exists

pip install onnxruntime numpyls src/models/emotion_bilstm_final.onnx



# Configure environment# 5. Start server

cp .env.example .envnpm start

# Edit .env with your API keys```



# Start backend server### Quick Test

npm run dev

``````bash

# Test text emotion

Backend will run on `http://localhost:3000`curl -X POST http://localhost:3000/api/analyze/text \

  -H "Content-Type: application/json" \

### 2. Setup Frontend  -d '{"text": "I am so happy today!"}'



```bash# Test voice emotion

# Navigate to frontendcurl -X POST http://localhost:3000/api/analyze/voice \

cd frontend  -F "audio=@audio.wav"

```

# Choose your framework and follow frontend/README.md

# Example with React + Vite:## ğŸ“¡ API Endpoints

npm create vite@latest . -- --template react-ts

npm install### 1. Text Emotion Analysis

npm install axios tailwindcss

Analyzes text using **BiLSTM ONNX + HuggingFace** in parallel.

# Start frontend dev server

npm run dev```http

```POST /api/analyze/text

Content-Type: application/json

Frontend will run on `http://localhost:5173` (Vite) or `http://localhost:3001` (configure as needed)

{

## ğŸ“¡ API Architecture  "text": "I'm feeling really happy today!",

  "userId": "user123"

### Backend API Endpoints}

```

| Endpoint | Method | Description |

|----------|--------|-------------|**Response:**

| `/api/analyze/text` | POST | Analyze text emotion (dual-model) |```json

| `/api/analyze/voice` | POST | Analyze voice emotion (multi-stage) |{

| `/api/analyze/multimodal` | POST | Combined text + voice analysis |  "success": true,

| `/api/response/generate` | POST | Generate AI empathetic response |  "data": {

| `/api/health` | GET | Health check |    "emotion": "happy",

    "confidence": 0.88,

### Text Emotion Pipeline    "models_used": ["bilstm_onnx", "huggingface"],

    "combination_strategy": "weighted_average",

```    "individual_results": {

Text Input â†’ BiLSTM ONNX (6 emotions) + HuggingFace (7 emotions)      "bilstm": {"emotion": "happy", "confidence": 0.85},

    â†“ (parallel execution)      "huggingface": {"emotion": "joy", "confidence": 0.91}

Weighted Combination (50/50)    },

    â†“    "scores": {...}

Combined Emotion Result  }

```}

```

### Voice Emotion Pipeline

### 2. Voice Emotion Analysis

```

Audio Input â†’ Groq Whisper (STT) â†’ TranscriptionComplete pipeline: **STT â†’ Text Emotion â†’ Voice Emotion â†’ Combined**

    â†“

Text Emotion (BiLSTM + HuggingFace) + Voice Emotion (Wav2Vec2)```http

    â†“POST /api/analyze/voice

Weighted FusionContent-Type: multipart/form-data

    â†“

Final Combined EmotionaudioFile: <binary .wav/.mp3/.ogg/.webm file>

```userId: user123

```

## ğŸ­ Emotion Models

**Response:**

### Text Analysis (Dual Model)```json

{

1. **BiLSTM ONNX** (Custom Model)  "success": true,

   - 6 emotions: angry, disgust, fear, happy, neutral, sad  "data": {

   - Speed: ~500ms    "transcript": "I'm feeling really happy today",

   - Format: ONNX (CPU optimized)    "transcriptConfidence": 0.95,

    "sttProvider": "groq",

2. **HuggingFace DistilRoBERTa**    "sttModel": "whisper-large-v3-turbo",

   - Model: `j-hartmann/emotion-english-distilroberta-base`    "textEmotion": {

   - 7 emotions: includes surprise      "emotion": "happy",

   - Speed: ~1-2s      "confidence": 0.88,

      "models_used": ["bilstm_onnx", "huggingface"]

### Voice Analysis (Multi-Stage)    },

    "voiceEmotion": {

1. **Groq Whisper** (Speech-to-Text)      "emotion": "happy",

   - Model: `whisper-large-v3-turbo`      "confidence": 0.83

   - Languages: 90+ supported    },

    "emotion": "happy",

2. **HuggingFace Wav2Vec2** (Voice Emotion)    "confidence": 0.86,

   - Model: `superb/wav2vec2-base-superb-er`    "emotionMethod": "combined-weighted"

   - Audio feature analysis  }

}

## ğŸ”§ Configuration```



### Backend Environment Variables### 3. Multi-Modal Analysis



See `backend/.env.example` for complete configuration:```http

POST /api/analyze/multimodal

```envContent-Type: multipart/form-data

# API Keys (Required)

GROQ_API_KEY=your_groq_keyaudioFile: <binary audio>

GEMINI_API_KEY=your_gemini_keytext: "Optional text"

HUGGINGFACE_API_KEY=your_hf_keyuserId: user123

```

# BiLSTM Model (6 emotions)

BILSTM_TEXT_ENABLED=true### 4. LLM Response Generation

BILSTM_LABELS=angry,disgust,fear,happy,neutral,sad

```http

# ServerPOST /api/response/generate

PORT=3000Content-Type: application/json

NODE_ENV=development

```{

  "emotion": "happy",

### Frontend Environment Variables  "context": "User expressed happiness",

  "includeAudio": true

Create `frontend/.env`:}

```

```env

# Backend API URL### 5. Health Check

VITE_API_BASE_URL=http://localhost:3000/api

``````http

GET /api/health

## ğŸ§ª Testing```



### Test Backend## ğŸ­ Emotion Models



```bash### Text Emotion (Dual Model)

cd backend

#### 1. BiLSTM ONNX (Custom Model)

# Test text analysis- **Format**: ONNX (optimized)

curl -X POST http://localhost:3000/api/analyze/text \- **Emotions**: 6 classes (angry, disgust, fear, happy, neutral, sad)

  -H "Content-Type: application/json" \- **Speed**: ~500ms

  -d '{"text": "I am so happy today!"}'- **Inference**: CPU via ONNX Runtime



# Test voice analysis#### 2. HuggingFace DistilRoBERTa

curl -X POST http://localhost:3000/api/analyze/voice \- **Model**: `j-hartmann/emotion-english-distilroberta-base`

  -F "audio=@test.wav"- **Emotions**: 7 classes (includes surprise)

- **Speed**: ~1-2s

# Health check- **API**: HuggingFace Inference

curl http://localhost:3000/api/health

```**Combination**: Both run in parallel, weighted 50/50 by default



### Test Frontend### Voice Emotion (Multi-Stage)



```bash#### 1. Groq Whisper (Speech-to-Text)

cd frontend- **Model**: `whisper-large-v3-turbo`

npm run dev- **Languages**: 90+ (English, Hindi, etc.)

# Open http://localhost:5173 in browser- **Provider**: Groq Cloud API

```

#### 2. Text Emotion on Transcript

## ğŸ“š Documentation- Uses dual-model approach (BiLSTM + HuggingFace)



- **[Backend Documentation](./backend/README.md)** - API details, endpoints, architecture#### 3. HuggingFace Wav2Vec2 (Voice Emotion)

- **[Frontend Documentation](./frontend/README.md)** - Setup, frameworks, UI guidelines- **Model**: `superb/wav2vec2-base-superb-er`

- **[Backend Detailed Docs](./backend/README_DETAILED.md)** - Comprehensive backend documentation- **Features**: Audio signal analysis

- **Inference**: Local Python

## ğŸ› ï¸ Development Workflow

## ğŸ”§ Configuration

### Running Both Services

### Environment Variables (.env)

**Terminal 1 - Backend:**

```bash```env

cd backend# Server

npm run devPORT=3000

```NODE_ENV=development



**Terminal 2 - Frontend:**# API Keys (Required)

```bashGROQ_API_KEY=your_groq_key_here

cd frontendGEMINI_API_KEY=your_gemini_key_here

npm run devHUGGINGFACE_API_KEY=your_hf_key_here

```

# BiLSTM ONNX Model (6 emotions)

### Adding FeaturesBILSTM_TEXT_ENABLED=true

BILSTM_MODEL_PATH=./src/models/emotion_bilstm_final.onnx

**Backend:**BILSTM_LABELS=angry,disgust,fear,happy,neutral,sad

1. Add routes in `backend/src/routes/`

2. Implement service logic in respective service folders# HuggingFace Models

3. Update API documentationTEXT_EMOTION_MODEL=j-hartmann/emotion-english-distilroberta-base

VOICE_EMOTION_MODEL=superb/wav2vec2-base-superb-er

**Frontend:**

1. Create components in `frontend/src/components/`# Groq Whisper

2. Add API calls in `frontend/src/services/`GROQ_MODEL=whisper-large-v3-turbo

3. Update UI/UX as neededSTT_LANGUAGE=en  # or 'hi' for Hindi, or omit for auto-detect



## ğŸš¨ Troubleshooting# LLM

LLAMA_ENABLED=true

### Backend IssuesLLAMA_MODEL=llama-3.3-70b-versatile



**"BiLSTM model fails"**# Multi-Modal Fusion Weights

- Ensure Python dependencies installed: `pip install onnxruntime numpy`TEXT_EMOTION_WEIGHT=0.5

- Check model exists: `ls backend/models/emotion_bilstm_final.onnx`VOICE_EMOTION_WEIGHT=0.5



**"API key errors"**# Database

- Verify all keys in `backend/.env`DATABASE_TYPE=supabase  # or 'sqlite'

- Check API rate limitsSUPABASE_URL=your_supabase_url

SUPABASE_ANON_KEY=your_key

### Frontend Issues

# TTS (Optional)

**"Cannot connect to backend"**TTS_ENABLED=false

- Ensure backend is running on port 3000```

- Check CORS settings in backend

- Verify API base URL in frontend `.env`## ğŸ“ Project Structure



**"Build errors"**```

- Clear node_modules: `rm -rf node_modules && npm install`emotion-detection-backend/

- Check Node.js version (18+)â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ server.js                          # Entry point

## ğŸ¯ Roadmapâ”‚   â”œâ”€â”€ config/

â”‚   â”‚   â””â”€â”€ index.js                       # Configuration

### Backendâ”‚   â”œâ”€â”€ routes/

- [x] Dual-model text emotionâ”‚   â”‚   â”œâ”€â”€ textRoutes.js                  # Text API

- [x] Multi-stage voice emotionâ”‚   â”‚   â”œâ”€â”€ voiceRoutes.js                 # Voice API

- [x] Multi-modal fusionâ”‚   â”‚   â”œâ”€â”€ multiModalRoutes.js            # Multi-modal API

- [x] LLM integration (Gemini + LLaMA)â”‚   â”‚   â””â”€â”€ healthRoutes.js                # Health check

- [x] ONNX optimizationâ”‚   â”œâ”€â”€ text-service/

- [ ] WebSocket support for real-timeâ”‚   â”‚   â”œâ”€â”€ index.js                       # Text emotion service

- [ ] Docker containerizationâ”‚   â”‚   â””â”€â”€ bilstm_onnx_inference.py       # BiLSTM ONNX inference

- [ ] Kubernetes deploymentâ”‚   â”œâ”€â”€ voice-service/

â”‚   â”‚   â”œâ”€â”€ index.js                       # Voice emotion service

### Frontendâ”‚   â”‚   â””â”€â”€ huggingface_emotion.py         # Voice emotion model

- [ ] Dashboard UIâ”‚   â”œâ”€â”€ multi-modal-layer/

- [ ] Text analysis interfaceâ”‚   â”‚   â””â”€â”€ index.js                       # Emotion fusion

- [ ] Voice recording + uploadâ”‚   â”œâ”€â”€ llm-service/

- [ ] Multi-modal interfaceâ”‚   â”‚   â””â”€â”€ index.js                       # Gemini/LLaMA

- [ ] History & analyticsâ”‚   â”œâ”€â”€ storage-service/

- [ ] Settings panelâ”‚   â”‚   â””â”€â”€ index.js                       # Data persistence

- [ ] Dark modeâ”‚   â”œâ”€â”€ models/

- [ ] Mobile responsiveâ”‚   â”‚   â””â”€â”€ emotion_bilstm_final.onnx      # BiLSTM model

- [ ] PWA supportâ”‚   â””â”€â”€ utils/

â”‚       â””â”€â”€ logger.js                      # Logging

## ğŸ¤ Contributingâ”œâ”€â”€ temp/audio/                            # Temp files

â”œâ”€â”€ data/                                  # SQLite DB

Contributions welcome! Please:â”œâ”€â”€ BILSTM_FIX.md                         # Fix guide

â”œâ”€â”€ SETUP_BILSTM.md                       # Setup guide

1. Fork the repositoryâ”œâ”€â”€ TESTING_GUIDE.md                      # Testing

2. Create feature branch (`git checkout -b feature/amazing-feature`)â”œâ”€â”€ QUICK_REFERENCE.md                    # Quick ref

3. Commit changes (`git commit -m 'Add amazing feature'`)â””â”€â”€ README.md                             # This file

4. Push to branch (`git push origin feature/amazing-feature`)```

5. Open Pull Request

## ğŸ§ª Testing

### Code Style

```bash

**Backend:** ES6 modules, async/await, descriptive naming# Test BiLSTM directly

python src/text-service/bilstm_onnx_inference.py \

**Frontend:** TypeScript, functional components, hooks  "src/models/emotion_bilstm_final.onnx" \

  "I am so happy" \

## ğŸ“„ License  "angry,disgust,fear,happy,neutral,sad"



ISC# Test APIs

curl -X POST http://localhost:3000/api/analyze/text \

## ğŸ™ Acknowledgments  -H "Content-Type: application/json" \

  -d '{"text": "I am happy"}'

- **Groq** - Fast Whisper API

- **HuggingFace** - Emotion detection modelscurl -X POST http://localhost:3000/api/analyze/voice \

- **Google** - Gemini LLM  -F "audio=@test.wav"

- **ONNX** - Optimized ML inference```



---## ğŸš¨ Troubleshooting



**Built with â¤ï¸ for emotion-aware AI applications**### BiLSTM Issues



For detailed documentation on specific components, see the respective README files in `backend/` and `frontend/` directories.**"index 6 is out of bounds"**

- âœ… Fixed! Model uses 6 emotions (not 7)
- See [BILSTM_FIX.md](./BILSTM_FIX.md)

**"onnxruntime not installed"**
```bash
pip install onnxruntime numpy
```

**"Model file not found"**
```bash
ls src/models/emotion_bilstm_final.onnx
```

### API Issues

**"Groq API key not configured"**
- Add `GROQ_API_KEY` to `.env`

**"HuggingFace API failed"**
- Verify `HUGGINGFACE_API_KEY` in `.env`
- Check internet connection
- May be rate-limited (wait/retry)

## ğŸ“š Documentation

### Setup & Testing
- [SETUP_BILSTM.md](./SETUP_BILSTM.md) - BiLSTM setup
- [BILSTM_FIX.md](./BILSTM_FIX.md) - Index error fix
- [TESTING_GUIDE.md](./TESTING_GUIDE.md) - Testing guide
- [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) - Quick reference

### Architecture
- [SUMMARY_CHANGES.md](./SUMMARY_CHANGES.md) - Change summary
- Service READMEs in each `src/*/README.md`

## ğŸ¯ Features

- [x] Dual-model text emotion (BiLSTM + HuggingFace)
- [x] Groq Whisper cloud STT
- [x] Voice emotion from audio
- [x] Multi-modal fusion
- [x] Gemini LLM + LLaMA fallback
- [x] ONNX optimization
- [x] Error handling & fallbacks
- [ ] Real-time streaming
- [ ] Docker containerization
- [ ] Monitoring dashboard

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Follow existing code style
4. Add tests and documentation
5. Submit Pull Request

## ğŸ“„ License

ISC

## ğŸ™ Acknowledgments

- **Groq** - Fast Whisper API
- **HuggingFace** - Emotion models
- **Google** - Gemini LLM
- **ONNX** - Optimized inference

---

**Built with â¤ï¸ for emotion-aware AI applications**
