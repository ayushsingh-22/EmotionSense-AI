# üß† EmotionSense AI (MantrAI) - Comprehensive Project Documentation

A full-stack **AI-powered emotion detection platform** combining **dual-model text analysis**, **advanced voice processing**, and **empathetic AI responses**. Built with modern web technologies for real-time emotion detection and intelligent conversation.

---

## üìã Table of Contents

1. [Project Overview](#project-overview)
2. [Tech Stack](#tech-stack)
3. [Architecture](#architecture)
4. [Features](#features)
5. [Project Structure](#project-structure)
6. [Setup & Installation](#setup--installation)
7. [Backend Services](#backend-services)
8. [Frontend Architecture](#frontend-architecture)
9. [API Endpoints](#api-endpoints)
10. [Database Schema](#database-schema)
11. [Emotion Detection Pipeline](#emotion-detection-pipeline)
12. [Configuration Guide](#configuration-guide)
13. [Development Workflows](#development-workflows)
14. [Deployment](#deployment)
15. [Troubleshooting](#troubleshooting)

---

## üéØ Project Overview

**EmotionSense AI** (MantrAI) is an intelligent emotion detection and empathetic response generation platform. It analyzes user emotions through:
- **Text Analysis** - BiLSTM ONNX + HuggingFace DistilRoBERTa dual models
- **Voice Analysis** - Groq Whisper STT + HuggingFace Wav2Vec2 + Paraformer
- **Multi-Modal Fusion** - Weighted combination of text and voice emotions
- **AI Responses** - Google Gemini 2.0 Flash with LLaMA 3.3 fallback
- **Chat Persistence** - Supabase PostgreSQL with session management

### Use Cases
- Mental health support chatbot
- Empathetic customer service
- Mood tracking application
- Emotion-aware voice assistants
- Research on emotion detection

---

## üõ† Tech Stack

### Backend
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Runtime** | Node.js 18+ | JavaScript runtime |
| **Framework** | Express.js | HTTP server & routing |
| **ML Models** | ONNX Runtime | BiLSTM inference (text) |
| **Audio Processing** | FFmpeg, WAV | Audio format conversion |
| **LLM** | Google Gemini + Groq LLaMA | Text generation |
| **STT** | Groq Whisper v3 | Speech-to-text |
| **TTS** | Google Cloud, Piper, Sarvam | Text-to-speech |
| **Emotion Models** | HuggingFace | DistilRoBERTa, Wav2Vec2 |
| **Database** | Supabase (PostgreSQL) | User data & chat history |
| **Cache** | In-memory | 5-min emotion cache |

### Frontend
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Framework** | Next.js 14 | React SSR/SSG |
| **Language** | TypeScript | Type safety |
| **Styling** | Tailwind CSS + Radix UI | UI components |
| **State Management** | Zustand + React Context | Global state |
| **API Client** | Axios | HTTP requests |
| **Database Client** | Supabase SDK | Real-time DB sync |
| **Voice Recording** | Web Audio API | Browser recording |
| **Audio Visualization** | WaveSurfer.js | Waveform display |
| **Analytics** | Custom Performance Monitor | Request tracking |

---

## üèó Architecture

### Data Flow Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   USER INPUT (Text/Voice)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                                  ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Text Input ‚îÇ               ‚îÇ  Voice Input     ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                               ‚îÇ
          ‚ñº                               ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Text Analysis   ‚îÇ          ‚îÇ Voice Processing    ‚îÇ
   ‚îÇ (Dual Model)    ‚îÇ          ‚îÇ Groq Whisper STT    ‚îÇ
   ‚îÇ                 ‚îÇ          ‚îÇ (Auto-language      ‚îÇ
   ‚îÇ ‚Ä¢ BiLSTM ONNX   ‚îÇ          ‚îÇ  detection)         ‚îÇ
   ‚îÇ ‚Ä¢ HuggingFace   ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ   DistilRoBERTa ‚îÇ                   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                   ‚îÇ  Transcription    ‚îÇ
            ‚îÇ                   ‚îÇ  (Multi-lingual)  ‚îÇ
            ‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                           ‚îÇ
            ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ          ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   Emotion Analysis Fusion      ‚îÇ
   ‚îÇ ‚Ä¢ Text Emotion Score           ‚îÇ
   ‚îÇ ‚Ä¢ Voice Emotion Score          ‚îÇ
   ‚îÇ ‚Ä¢ Weighted Combination         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Final Emotion Result    ‚îÇ
   ‚îÇ ‚Ä¢ Detected Emotion       ‚îÇ
   ‚îÇ ‚Ä¢ Confidence Score       ‚îÇ
   ‚îÇ ‚Ä¢ All Model Scores       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  LLM Response Generation            ‚îÇ
   ‚îÇ  (with Fallback Chain)              ‚îÇ
   ‚îÇ ‚Ä¢ Primary: Google Gemini 2.0 Flash  ‚îÇ
   ‚îÇ ‚Ä¢ Fallback: LLaMA 3.3 via Groq     ‚îÇ
   ‚îÇ ‚Ä¢ Context: Chat History (5-10 msgs) ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Response Enhancement         ‚îÇ
   ‚îÇ ‚Ä¢ Optional TTS Synthesis      ‚îÇ
   ‚îÇ ‚Ä¢ Multi-language Support      ‚îÇ
   ‚îÇ ‚Ä¢ Empathetic Tone             ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Database Persistence            ‚îÇ
   ‚îÇ  (Supabase PostgreSQL)           ‚îÇ
   ‚îÇ ‚Ä¢ Save Session                   ‚îÇ
   ‚îÇ ‚Ä¢ Save Message History           ‚îÇ
   ‚îÇ ‚Ä¢ Save Emotion Analytics         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Frontend Display           ‚îÇ
   ‚îÇ ‚Ä¢ Chat UI                   ‚îÇ
   ‚îÇ ‚Ä¢ Emotion Badge             ‚îÇ
   ‚îÇ ‚Ä¢ Audio Playback (TTS)      ‚îÇ
   ‚îÇ ‚Ä¢ Typing Indicator          ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Service Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         EXPRESS SERVER                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ CORS Middleware       ‚Ä¢ Error Handler      ‚Ä¢ Request Logger   ‚îÇ
‚îÇ ‚Ä¢ Compression           ‚Ä¢ Upload Handler     ‚Ä¢ Request Limiter  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ                 ‚îÇ                 ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ TEXT SERVICE  ‚îÇ  ‚îÇVOICE SERVICE ‚îÇ  ‚îÇ  LLM SERVICE   ‚îÇ
       ‚îÇ               ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ                ‚îÇ
       ‚îÇ ‚Ä¢ BiLSTM ONNX ‚îÇ  ‚îÇ‚Ä¢ Groq STT    ‚îÇ  ‚îÇ‚Ä¢ Gemini        ‚îÇ
       ‚îÇ ‚Ä¢ HuggingFace ‚îÇ  ‚îÇ‚Ä¢ Wav2Vec2    ‚îÇ  ‚îÇ‚Ä¢ Groq LLaMA    ‚îÇ
       ‚îÇ ‚Ä¢ 5-min Cache ‚îÇ  ‚îÇ‚Ä¢ Features    ‚îÇ  ‚îÇ‚Ä¢ Fallback      ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                ‚îÇ                  ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ           MULTI-MODAL FUSION LAYER                 ‚îÇ
       ‚îÇ  ‚Ä¢ Weighted Combination (0.5 text + 0.5 voice)     ‚îÇ
       ‚îÇ  ‚Ä¢ Confidence Scoring                              ‚îÇ
       ‚îÇ  ‚Ä¢ Result Normalization                            ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  TTS SERVICE          ‚îÇ
       ‚îÇ                       ‚îÇ
       ‚îÇ ‚Ä¢ Google TTS (primary)‚îÇ
       ‚îÇ ‚Ä¢ Piper (fallback)    ‚îÇ
       ‚îÇ ‚Ä¢ Sarvam (fallback)   ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  STORAGE SERVICE         ‚îÇ
       ‚îÇ (Supabase ORM Layer)      ‚îÇ
       ‚îÇ                          ‚îÇ
       ‚îÇ ‚Ä¢ Sessions               ‚îÇ
       ‚îÇ ‚Ä¢ Messages               ‚îÇ
       ‚îÇ ‚Ä¢ User Profiles          ‚îÇ
       ‚îÇ ‚Ä¢ Analytics              ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚ú® Key Features

### üé≠ Emotion Detection
- **Dual-Model Text Analysis**: Combines BiLSTM ONNX + HuggingFace for robust predictions
- **Voice Emotion**: Multi-stage pipeline with STT ‚Üí Text Analysis ‚Üí Voice Feature Analysis
- **Multi-Modal Fusion**: Intelligent weighted averaging of text and voice emotions
- **Confidence Scoring**: Detailed confidence metrics for all predictions
- **Caching**: 5-minute cache prevents redundant API calls

### ü§ñ AI Response Generation
- **Primary LLM**: Google Gemini 2.0 Flash (state-of-the-art)
- **Fallback Chain**: LLaMA 3.3 70B via Groq if Gemini fails
- **Context-Aware**: Reads last 5-10 messages for coherent conversations
- **Empathetic Prompts**: Customized based on detected emotion
- **Language Support**: Automatic language detection and response in user's language

### üé§ Voice Processing
- **STT**: Groq Whisper v3 Turbo (multi-language, auto-detect)
- **Audio Features**: Extracts prosody, pitch, energy for voice emotion
- **TTS**: Multi-provider (Google TTS ‚Üí Piper ‚Üí Sarvam) with fallbacks
- **Format Support**: WAV, MP3, OPUS, OGG

### üí¨ Chat System
- **Session Management**: Persistent chat sessions with unique IDs
- **Message History**: Stored in Supabase with user association
- **Real-time Updates**: WebSocket support for live chat
- **Multi-turn Conversations**: Context window for coherent dialogue
- **User Isolation**: Row-Level Security on Supabase

### üé® Frontend Features
- **Responsive UI**: Mobile-friendly design with Tailwind CSS
- **Dark Mode**: Theme toggle with automatic preference detection
- **Voice Recording**: Browser-native recording with permission handling
- **Waveform Visualization**: Real-time audio visualization
- **Chat History**: Searchable message history with emotion filters
- **Performance Monitoring**: Track API response times

### üîê Security
- **Supabase RLS**: Row-Level Security for data isolation
- **JWT Authentication**: Secure user sessions
- **CORS Configuration**: Controlled cross-origin access
- **Input Validation**: Server-side text and audio validation
- **Rate Limiting**: (Configurable) API request throttling

---

## üìÅ Project Structure

```
emotion-sense-ai/
‚îú‚îÄ‚îÄ backend/                              # Node.js Backend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.js                    # Main Express entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # Centralized configuration
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ indianLanguages.js       # Language code mappings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errorHandler.js          # Global error middleware
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requestLogger.js         # HTTP request logging
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ uploadMiddleware.js      # Multer file upload config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chatRoutes.js            # Chat session endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ textRoutes.js            # Text emotion endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ voiceRoutes.js           # Voice emotion endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiModalRoutes.js      # Combined emotion endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ responseRoutes.js        # LLM response endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ttsRoutes.js             # Text-to-speech endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ healthRoutes.js          # Health check endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text-service/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # Text emotion analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bilstm_onnx_inference.py # BiLSTM Python wrapper
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Service documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ voice-service/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # Voice emotion analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotion_inference.py     # Voice feature extraction
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ huggingface_emotion.py   # HuggingFace wrapper
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Service documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm-service/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # LLM response generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Service documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tts-service/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # Text-to-speech synthesis
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Service documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage-service/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # Supabase ORM layer
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Service documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aggregator/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # Emotion fusion logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Service documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi-modal-layer/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js                 # Multi-modal processing
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Service documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ helpers.js               # Utility functions
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.js                # Logging utility
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ translationHelper.js     # Translation utilities
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ voiceHelper.js           # Voice utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ emotion_bilstm_final.onnx # BiLSTM model file
‚îÇ   ‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ add_audio_features_column.sql # Database migrations
‚îÇ   ‚îú‚îÄ‚îÄ .env                             # Environment variables
‚îÇ   ‚îú‚îÄ‚îÄ .env.example                     # Environment template
‚îÇ   ‚îú‚îÄ‚îÄ package.json                     # Dependencies
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ test-*.js                        # Test scripts
‚îÇ   ‚îî‚îÄ‚îÄ README_DETAILED.md               # Detailed backend docs
‚îÇ
‚îú‚îÄ‚îÄ frontend/                            # Next.js 14 Frontend
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx                   # Root layout with providers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                     # Home page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login/page.tsx           # Login page
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ signup/page.tsx          # Registration page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                 # Chat interface
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enhanced-page.tsx        # Enhanced chat version
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page-optimized.tsx       # Optimized version
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text/page.tsx                # Text analysis page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ voice/page.tsx               # Voice analysis page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multimodal/page.tsx          # Multi-modal analysis page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ history/page.tsx             # Chat history page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile/page.tsx             # User profile page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings/page.tsx            # Settings page
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ globals.css                  # Global styles
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ navbar.tsx                   # Navigation bar
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sidebar.tsx                  # Side navigation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MainContent.tsx              # Main content wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LoadingStates.tsx            # Loading skeletons
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ theme-provider.tsx           # Theme wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ theme-toggle.tsx             # Dark mode toggle
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuthGuard.tsx            # Route protection
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DeleteConfirmationDialog.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatLayout.tsx           # Chat container
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatSidebar.tsx          # Chat session list
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatMessage.tsx          # Message component
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatBubble.tsx           # Message bubble
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInput.tsx            # Message input
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EnhancedChatInput.tsx    # Rich input
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EnhancedVoiceControls.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SimpleVoiceRecorder.tsx  # Voice recorder
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TypingIndicator.tsx      # Typing animation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotions/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [emotion-components]    # Emotion display
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ voice/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [voice-components]      # Voice controls
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ui/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ button.tsx               # Button component
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ input.tsx                # Input field
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ card.tsx                 # Card layout
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dialog.tsx               # Modal dialog
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ toast.tsx                # Toast notifications
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ [other-ui-components]
‚îÇ   ‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuthContext.tsx              # Auth state & methods
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatContext.tsx              # Chat state & methods
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SidebarContext.tsx           # Sidebar state
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ use-toast.ts                 # Toast hook
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useVoiceRecorder.ts          # Voice recording logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useVoiceRecording.ts         # Alternative recording
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.ts                       # Axios API client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supabase.ts                  # Supabase client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance.ts               # Performance monitoring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.ts                     # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ store/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useStore.ts                  # Zustand global store
‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts                     # TypeScript interfaces
‚îÇ   ‚îú‚îÄ‚îÄ styles/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ globals.css                  # Global styles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat-enhancements.css        # Chat styling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat-optimized.css           # Optimized styling
‚îÇ   ‚îú‚îÄ‚îÄ package.json                     # Dependencies
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json                    # TypeScript config
‚îÇ   ‚îú‚îÄ‚îÄ tailwind.config.ts               # Tailwind config
‚îÇ   ‚îú‚îÄ‚îÄ postcss.config.mjs               # PostCSS config
‚îÇ   ‚îî‚îÄ‚îÄ next.config.mjs                  # Next.js config
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ copilot-instructions.md          # Project guidelines
‚îÇ
‚îî‚îÄ‚îÄ README.md                            # This file

```

---

## üöÄ Setup & Installation

### Prerequisites

```
‚úì Node.js 18 or higher (npm 9+)
‚úì Python 3.8+ (for ONNX inference)
‚úì Git
‚úì API Keys:
  - Google Gemini API
  - Groq API (for LLaMA & Whisper)
  - HuggingFace API Token
  - Supabase Project
  - Google TTS API (optional)
```

### Backend Setup

```bash
# 1. Navigate to backend directory
cd backend

# 2. Install Node dependencies
npm install

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Create environment configuration
cp .env.example .env

# 5. Edit .env with your API keys
nano .env
# (See Configuration Guide below)

# 6. Verify ONNX model file
ls -la src/models/emotion_bilstm_final.onnx

# 7. Start development server
npm run dev

# Backend runs on: http://localhost:8080
```

### Frontend Setup

```bash
# 1. Navigate to frontend directory
cd frontend

# 2. Install Node dependencies
npm install

# 3. Create environment file
cp .env.local.example .env.local

# 4. Configure environment variables
# Edit .env.local with Supabase and API URLs

# 5. Start development server
npm run dev

# Frontend runs on: http://localhost:3000
```

### Full Stack Quick Start

```bash
# From project root

# Terminal 1: Backend
cd backend && npm run dev

# Terminal 2: Frontend
cd frontend && npm run dev

# Terminal 3 (optional): Run tests
cd backend && npm run test:basic
```

### Verification Checklist

- [ ] Backend server starts without errors (Port 8080)
- [ ] Frontend builds successfully (Port 3000)
- [ ] ONNX model file exists in `backend/src/models/`
- [ ] All environment variables are set
- [ ] Supabase connection established
- [ ] API endpoints respond to health checks

```bash
# Test endpoints
curl http://localhost:8080/api/health
curl http://localhost:3000/api/health
```

---

## üîß Backend Services

### 1. **Text Service** (`src/text-service/`)
Dual-model text emotion analysis combining BiLSTM and HuggingFace.

**Models:**
- BiLSTM ONNX: 6 emotions (angry, disgust, fear, happy, neutral, sad)
- HuggingFace DistilRoBERTa: 7 emotions (adds surprise)

**Features:**
- Parallel model execution
- 5-minute caching
- Weighted fusion (50/50 by default)
- Confidence scoring

**Input:** `{ text: string }`

**Output:**
```json
{
  "emotion": "happy",
  "confidence": 0.92,
  "scores": {
    "angry": 0.02,
    "disgust": 0.01,
    "fear": 0.02,
    "happy": 0.92,
    "neutral": 0.02,
    "sad": 0.01
  },
  "individual_results": {
    "bilstm": { "emotion": "happy", "confidence": 0.95, "scores": {} },
    "huggingface": { "emotion": "happy", "confidence": 0.89, "scores": {} }
  }
}
```

---

### 2. **Voice Service** (`src/voice-service/`)
Multi-stage voice emotion analysis with transcription.

**Pipeline:**
1. Audio Upload ‚Üí Groq Whisper STT (auto language detection)
2. Transcription ‚Üí Text Emotion Analysis
3. Audio Features ‚Üí HuggingFace Wav2Vec2 Voice Emotion
4. Fusion: Weight text (60%) + voice (40%)

**Features:**
- Multi-language support
- Audio format conversion (FFmpeg)
- Feature extraction (MFCC, prosody)
- Fallback providers for STT

**Input:** Audio file (WAV, MP3, OPUS)

**Output:**
```json
{
  "transcript": "I am feeling great today",
  "detected_language": "en",
  "text_emotion": { "emotion": "happy", "confidence": 0.88 },
  "voice_emotion": { "emotion": "happy", "confidence": 0.85 },
  "combined_emotion": { "emotion": "happy", "confidence": 0.867 }
}
```

---

### 3. **LLM Service** (`src/llm-service/`)
Empathetic response generation with fallback chain.

**Primary Model:** Google Gemini 2.0 Flash (fast, multimodal)
**Fallback:** LLaMA 3.3 70B via Groq

**Features:**
- Context-aware prompting (last 5-10 messages)
- Empathetic tone based on detected emotion
- Fallback to Groq LLaMA if Gemini fails
- API key rotation support
- Temperature control (default 0.7)

**Function Signature:**
```javascript
generateResponse({
  message: string,
  emotion: string,
  confidence: number,
  chatHistory?: Array,
  userId?: string
})
```

**Prompt Template:**
```
The user is feeling {emotion} (confidence: {confidence}%).
Recent context: {chatHistory}

Respond empathetically and conversationally. Reference the chat history
if relevant. Keep response concise (2-3 sentences).
```

---

### 4. **TTS Service** (`src/tts-service/`)
Multi-provider text-to-speech synthesis.

**Providers:**
1. Google Cloud TTS (primary) - Supports 100+ languages, Neural voices
2. Piper TTS (fallback) - Open-source, fast
3. Sarvam TTS (fallback) - Indian language support
4. OpenAI TTS (fallback) - High quality

**Features:**
- Language auto-detection from text
- Voice selection based on gender/language
- MP3 or WAV output
- Configurable speaking rate & pitch

**Input:** `{ text: string, language?: string }`

**Output:** `{ audioUrl: string, contentType: string }`

---

### 5. **Storage Service** (`src/storage-service/`)
Supabase ORM layer for persistent data storage.

**Tables:**
- `users`: User profiles
- `chat_sessions`: Chat session metadata
- `messages`: Individual chat messages
- `emotions`: Emotion analysis logs

**Features:**
- Row-Level Security (RLS) for user isolation
- Transaction support for atomic operations
- Automatic timestamps and user tracking
- Query optimization with proper indexing

**Key Functions:**
```javascript
// Sessions
createSession(userId, topic)
getSession(sessionId)
updateSession(sessionId, data)
listSessions(userId)

// Messages
saveMessage(sessionId, role, content, emotion)
getMessages(sessionId, limit)
deleteMessage(messageId)

// Users
createUser(userData)
updateUserProfile(userId, data)
getUserAnalytics(userId)
```

---

### 6. **Aggregator** (`src/aggregator/`)
Weighted emotion fusion for multi-modal analysis.

**Fusion Logic:**
```
Combined Emotion = (text_emotion_score √ó text_weight) + (voice_emotion_score √ó voice_weight)

Default weights:
- Text: 50%
- Voice: 50%

Custom weights can be configured per use case
```

**Features:**
- Confidence-based weighting
- Outlier detection
- Emotion conflict resolution
- Detailed score tracking

---

### 7. **Multi-Modal Layer** (`src/multi-modal-layer/`)
Orchestrates text, voice, and data services.

**Responsibilities:**
- Request routing
- Service coordination
- Error handling & fallbacks
- Response normalization

---

## üé® Frontend Architecture

### State Management

**1. React Context API**

```typescript
// AuthContext - User authentication
const { user, isLoading, login, signup, logout, profile } = useAuth();

// ChatContext - Chat state
const { messages, sessionId, isLoading, sendMessage, newSession } = useChat();

// SidebarContext - UI state
const { isOpen, toggle } = useSidebar();
```

**2. Zustand Store**

```typescript
// Global store with localStorage persistence
const { history, addAnalysis, clearHistory, preferences } = useStore();

// Features:
// - Capped at 25 analyses
// - localStorage sync
// - Immer middleware for immutable updates
```

### Component Architecture

```
App
‚îú‚îÄ‚îÄ Layout (with providers)
‚îÇ   ‚îú‚îÄ‚îÄ Navbar
‚îÇ   ‚îú‚îÄ‚îÄ Sidebar
‚îÇ   ‚îî‚îÄ‚îÄ MainContent
‚îÇ       ‚îî‚îÄ‚îÄ Page Route
‚îÇ
‚îú‚îÄ‚îÄ Chat Page
‚îÇ   ‚îú‚îÄ‚îÄ ChatLayout
‚îÇ   ‚îú‚îÄ‚îÄ ChatSidebar (session list)
‚îÇ   ‚îú‚îÄ‚îÄ ChatMessage (individual messages)
‚îÇ   ‚îú‚îÄ‚îÄ ChatInput (text + voice)
‚îÇ   ‚îî‚îÄ‚îÄ TypingIndicator
‚îÇ
‚îú‚îÄ‚îÄ Text Analysis Page
‚îÇ   ‚îú‚îÄ‚îÄ TextInput
‚îÇ   ‚îî‚îÄ‚îÄ EmotionResult
‚îÇ
‚îú‚îÄ‚îÄ Voice Analysis Page
‚îÇ   ‚îú‚îÄ‚îÄ VoiceRecorder
‚îÇ   ‚îú‚îÄ‚îÄ Waveform
‚îÇ   ‚îî‚îÄ‚îÄ EmotionResult
‚îÇ
‚îú‚îÄ‚îÄ MultiModal Page
‚îÇ   ‚îú‚îÄ‚îÄ TextInput
‚îÇ   ‚îú‚îÄ‚îÄ VoiceRecorder
‚îÇ   ‚îî‚îÄ‚îÄ CombinedResult
‚îÇ
‚îú‚îÄ‚îÄ History Page
‚îÇ   ‚îî‚îÄ‚îÄ AnalysisTable (filterable, sortable)
‚îÇ
‚îî‚îÄ‚îÄ Settings Page
    ‚îú‚îÄ‚îÄ ThemeToggle
    ‚îú‚îÄ‚îÄ LanguageSelect
    ‚îî‚îÄ‚îÄ PreferencesForm
```

### API Client Pattern

```typescript
// lib/api.ts - Centralized API calls
export const analyzeText = async (text: string) => {
  return withCache(`text-${text.slice(0, 50)}`, async () => {
    const response = await api.post('/analyze/text', { text });
    return transformResponse(response.data);
  });
};

// Features:
// - Request/response caching
// - Error handling & retry logic
// - Performance monitoring
// - Type-safe responses
```

### Performance Optimizations

1. **Dynamic Imports**: Navbar and heavy components
2. **Response Caching**: 5-minute cache for identical inputs
3. **Debouncing**: Profile fetch, text input (300ms)
4. **Memoization**: Prevent unnecessary re-renders
5. **Code Splitting**: Next.js automatic route-based splitting
6. **Image Optimization**: Next.js Image component

---

## üì° API Endpoints

### Authentication (Supabase)
```
POST   /auth/signup           - Register new user
POST   /auth/login            - Login user
POST   /auth/logout           - Logout user
GET    /auth/profile          - Get user profile
```

### Text Analysis
```
POST   /api/analyze/text
Content-Type: application/json

Request:
{
  "text": "I am feeling great!",
  "includeIndividualResults": true
}

Response: TextAnalysisResult
```

### Voice Analysis
```
POST   /api/analyze/voice
Content-Type: multipart/form-data

Request:
- audio: <audio-file>

Response: VoiceAnalysisResult
```

### Chat System
```
POST   /api/chat/message
Content-Type: application/json
Authorization: Bearer <JWT>

Request:
{
  "message": "Hello, how are you?",
  "sessionId": "optional-uuid",
  "memoryLength": 10
}

Response:
{
  "response": "AI response text",
  "emotion": "happy",
  "sessionId": "uuid",
  "audioUrl": "/api/tts/audio/file.wav",
  "messageId": "uuid"
}
```

### Chat Sessions
```
GET    /api/chat/sessions         - List user sessions
GET    /api/chat/sessions/:id     - Get session details
POST   /api/chat/sessions         - Create new session
DELETE /api/chat/sessions/:id     - Delete session
```

### Response Generation
```
POST   /api/response-generator
Content-Type: application/json

Request:
{
  "message": "I am sad",
  "emotion": "sad",
  "confidence": 0.85,
  "chatHistory": []
}

Response:
{
  "response": "I understand you're feeling sad...",
  "model": "gemini-2.0-flash",
  "tokens": { "input": 25, "output": 50 }
}
```

### Text-to-Speech
```
POST   /api/tts/synthesize
Content-Type: application/json

Request:
{
  "text": "I am feeling happy",
  "language": "en-US",
  "voiceGender": "female"
}

Response:
{
  "audioUrl": "/api/tts/audio/file.wav",
  "provider": "google",
  "duration": 2.5
}
```

### Health & Diagnostics
```
GET    /api/health              - Server status
GET    /api/health/services     - All service status
GET    /api/health/models       - ML model status
```

---

## üóÑ Database Schema

### Supabase PostgreSQL Tables

#### `users`
```sql
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  auth_id UUID UNIQUE REFERENCES auth.users(id),
  email VARCHAR(255) UNIQUE NOT NULL,
  full_name VARCHAR(255),
  avatar_url TEXT,
  preferred_language VARCHAR(10) DEFAULT 'en',
  theme_preference VARCHAR(10) DEFAULT 'system',
  tts_enabled BOOLEAN DEFAULT true,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);
```

#### `chat_sessions`
```sql
CREATE TABLE chat_sessions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  title VARCHAR(255),
  topic VARCHAR(100),
  emotion_context TEXT,
  message_count INTEGER DEFAULT 0,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);
```

#### `messages`
```sql
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID NOT NULL REFERENCES chat_sessions(id) ON DELETE CASCADE,
  role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant')),
  content TEXT NOT NULL,
  emotion VARCHAR(50),
  emotion_confidence FLOAT,
  audio_url TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);
```

#### `emotions`
```sql
CREATE TABLE emotions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  session_id UUID REFERENCES chat_sessions(id) ON DELETE CASCADE,
  emotion VARCHAR(50) NOT NULL,
  confidence FLOAT,
  model_used VARCHAR(100),
  input_type VARCHAR(20) CHECK (input_type IN ('text', 'voice', 'multimodal')),
  created_at TIMESTAMP DEFAULT NOW()
);
```

### Row-Level Security (RLS) Policies

```sql
-- Users can only access their own data
ALTER TABLE chat_sessions ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their own sessions"
  ON chat_sessions FOR SELECT
  USING (auth.uid() = user_id);

CREATE POLICY "Users can create sessions"
  ON chat_sessions FOR INSERT
  WITH CHECK (auth.uid() = user_id);
```

---

## üß† Emotion Detection Pipeline (Detailed)

### 1. Text Emotion Detection

**Step 1: Input Processing**
- Normalize text (lowercase, remove extra spaces)
- Check 5-minute cache for exact match
- Return cached result if found

**Step 2: Parallel Model Inference**

**BiLSTM ONNX Model:**
- Input: Tokenized text
- Model: emotion_bilstm_final.onnx
- Output: 6 emotion logits
- Labels: angry, disgust, fear, happy, neutral, sad

**HuggingFace DistilRoBERTa:**
- Model: `michellejieli/emotion_text_classifier`
- Output: 7 emotion scores (includes surprise)
- Provider: HuggingFace Inference API

**Step 3: Weighted Fusion**
```javascript
combinedScores = {};
for (emotion in allEmotions) {
  score = (bilstmScore[emotion] * 0.5) + (hfScore[emotion] * 0.5);
  combinedScores[emotion] = score;
}
finalEmotion = max(combinedScores);
confidence = combinedScores[finalEmotion];
```

**Step 4: Cache Result**
- Store in memory for 5 minutes
- Key: hash(text)

---

### 2. Voice Emotion Detection

**Step 1: Speech-to-Text (Groq Whisper)**
```
Audio File (any format)
    ‚Üì
FFmpeg Conversion ‚Üí WAV format
    ‚Üì
Groq Whisper v3 Turbo API
    ‚Üì
Text Transcription + Language Detection
```

**Step 2: Text Emotion from Transcription**
- Run text emotion pipeline on transcript
- Confidence adjusted by transcription quality

**Step 3: Voice Feature Extraction**
```
Audio Analysis:
- MFCC (Mel-Frequency Cepstral Coefficients)
- Prosody (pitch, energy, speaking rate)
- Pause duration
- Voice quality metrics
```

**Step 4: Voice Emotion via Wav2Vec2**
- Model: `superb/wav2vec2-base-superb-er`
- Input: Audio features + waveform
- Output: Emotion classification

**Step 5: Multi-Modal Fusion**
```
Voice Emotion Score = (transcription_emotion * 0.6) + (audio_emotion * 0.4)
```

---

### 3. Multi-Modal Fusion

**Scenario 1: Text + Voice (equal confidence)**
```
Final Emotion = (text_emotion * 0.5) + (voice_emotion * 0.5)
```

**Scenario 2: Text + Voice (confidence-based)**
```
text_weight = text_confidence / (text_confidence + voice_confidence)
voice_weight = voice_confidence / (text_confidence + voice_confidence)

Final Emotion = (text_emotion * text_weight) + (voice_emotion * voice_weight)
```

**Scenario 3: Conflicting emotions**
```
if (text_emotion != voice_emotion) {
  // Choose based on higher confidence
  // Or use emotion similarity scoring
  // Or average neutral ground
}
```

---

## ‚öôÔ∏è Configuration Guide

### Backend .env File

```env
# ============================================
# SERVER CONFIGURATION
# ============================================
PORT=8080
NODE_ENV=development
CORS_ORIGIN=http://localhost:3000

# ============================================
# GEMINI API (Primary LLM)
# ============================================
GEMINI_API_KEY1=your-key-here
GEMINI_API_KEY2=backup-key-here
GEMINI_MAX_TOKENS=1024
GEMINI_TEMPERATURE=0.7

# ============================================
# GROQ API (LLaMA + Whisper STT)
# ============================================
GROQ_API_KEY=your-groq-key
GROQ_MODEL=whisper-large-v3-turbo
STT_LANGUAGE=  # Leave empty for auto-detect

# ============================================
# HUGGINGFACE (Emotion Models)
# ============================================
HUGGINGFACE_API_KEY=your-hf-token
TEXT_EMOTION_MODEL=michellejieli/emotion_text_classifier
VOICE_EMOTION_MODEL=superb/wav2vec2-base-superb-er

# ============================================
# TEXT-TO-SPEECH
# ============================================
TTS_ENABLED=true
TTS_PROVIDER=google  # google|piper|sarvam|openai
GOOGLE_TTS_API_KEY=your-google-tts-key
GOOGLE_TTS_VOICE=en-US-Neural2-C
GOOGLE_TTS_SPEED=1.0

# ============================================
# SUPABASE DATABASE
# ============================================
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-service-role-key

# ============================================
# ONNX MODEL
# ============================================
BILSTM_TEXT_ENABLED=true
BILSTM_MODEL_PATH=./src/models/emotion_bilstm_final.onnx
BILSTM_LABELS=angry,disgust,fear,happy,neutral,sad
```

### Frontend .env.local File

```env
# ============================================
# API CONFIGURATION
# ============================================
NEXT_PUBLIC_API_URL=http://localhost:8080

# ============================================
# SUPABASE
# ============================================
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key

# ============================================
# FEATURES
# ============================================
NEXT_PUBLIC_ENABLE_VOICE=true
NEXT_PUBLIC_ENABLE_TTS=true
NEXT_PUBLIC_ENABLE_CHAT_PERSISTENCE=true
```

---

## üíª Development Workflows

### Adding a New Emotion Detection Model

**1. Implement Model Service**
```javascript
// backend/src/your-service/index.js
export const analyzeWithNewModel = async (input) => {
  // Model inference logic
};
```

**2. Update Fusion Layer**
```javascript
// backend/src/aggregator/index.js
const newModelResult = await analyzeWithNewModel(input);
scores.newModel = newModelResult.scores;
```

**3. Update Types**
```typescript
// frontend/types/index.ts
export interface TextAnalysisResult {
  individual_results?: {
    // ... existing models
    newModel?: ModelResult;
  };
}
```

---

### Adding a New LLM Provider

**1. Implement Provider Function**
```javascript
// backend/src/llm-service/index.js
async function generateResponseWithNewProvider(message, context) {
  // API call to new provider
}
```

**2. Add to Fallback Chain**
```javascript
export async function generateResponse(message, emotion) {
  try {
    return await generateResponseWithGemini(message, emotion);
  } catch (error) {
    try {
      return await generateResponseWithGroqLLaMA(message, emotion);
    } catch (error2) {
      return await generateResponseWithNewProvider(message, emotion);
    }
  }
}
```

---

### Testing Emotion Detection

```bash
# Test text emotion endpoint
curl -X POST http://localhost:8080/api/analyze/text \
  -H "Content-Type: application/json" \
  -d '{"text": "I am so happy today!"}'

# Test voice emotion endpoint
curl -X POST http://localhost:8080/api/analyze/voice \
  -F "audio=@test_audio.wav"

# Test chat endpoint
curl -X POST http://localhost:8080/api/chat/message \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "message": "I had a bad day",
    "sessionId": "session-123"
  }'
```

---

### Running Tests

```bash
# Backend tests
cd backend

# Test basic endpoints
npm run test:basic

# Test live API (full pipeline)
npm run test:live

# Test translation service
npm run test:translation
```

---

### Frontend Development Tips

**1. Environment Variables**
- Always prefix with `NEXT_PUBLIC_` for browser access
- Create `.env.local` for local development

**2. Type Safety**
- Define interfaces in `types/index.ts`
- Use TypeScript strict mode
- Avoid `any` type

**3. State Management**
- Use Zustand for global state
- Use Context for auth/chat
- Local component state for UI

**4. Performance**
- Use `React.memo()` for expensive components
- Implement `useMemo()` for heavy computations
- Use dynamic imports for large components
- Cache API responses

---

## üöÄ Deployment

### Backend Deployment (Heroku/Railway/Render)

```bash
# 1. Create Procfile
echo "web: npm start" > Procfile

# 2. Add buildpacks
heroku buildpacks:add heroku/nodejs
heroku buildpacks:add heroku/python

# 3. Set environment variables
heroku config:set GEMINI_API_KEY=your-key
heroku config:set GROQ_API_KEY=your-key
# ... etc

# 4. Deploy
git push heroku main
```

### Frontend Deployment (Vercel)

```bash
# 1. Push to GitHub
git push origin main

# 2. Connect to Vercel
# Go to https://vercel.com/new
# Select repository
# Add environment variables from .env.local

# 3. Deploy
# Automatic on push to main
```

### Docker Containerization

```dockerfile
# Backend Dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY src ./src

EXPOSE 8080

CMD ["npm", "start"]
```

```bash
# Build and run
docker build -t emotion-sense-backend .
docker run -p 8080:8080 emotion-sense-backend
```

---

## üêõ Troubleshooting

### Common Issues & Solutions

#### 1. **ONNX Model Not Found**
```
Error: Cannot find module 'emotion_bilstm_final.onnx'
```
**Solution:**
- Verify file exists: `ls backend/src/models/emotion_bilstm_final.onnx`
- Check permissions: `chmod 644 src/models/*.onnx`
- Download model if missing: [Download Link]

---

#### 2. **HuggingFace Rate Limited**
```
Error: Rate limit exceeded from HuggingFace API
```
**Solution:**
- Text emotion results are cached for 5 minutes
- Wait 1-2 minutes before retrying
- Batch requests where possible
- Upgrade HuggingFace account for higher limits

---

#### 3. **Gemini API Key Invalid**
```
Error: 403 Permission denied from Gemini API
```
**Solution:**
- Verify API key in `.env`
- Check API key has correct permissions
- Ensure quota not exceeded
- Use backup key: `GEMINI_API_KEY2`

---

#### 4. **Supabase Connection Failed**
```
Error: Failed to connect to Supabase
```
**Solution:**
- Check `SUPABASE_URL` and `SUPABASE_KEY` in `.env`
- Verify network connectivity
- Ensure Supabase project is running
- Check Row-Level Security policies

---

#### 5. **Voice Recording Permission Denied**
```
Error: NotAllowedError: Permission denied
```
**Solution:**
- HTTPS required for production (localhost OK)
- Check browser microphone permissions
- Request permission explicitly in app
- Try different browser

---

#### 6. **Frontend Can't Connect to Backend**
```
Error: Failed to fetch from http://localhost:8080
```
**Solution:**
- Backend must be running on port 8080
- Check CORS configuration in backend
- Verify `NEXT_PUBLIC_API_URL` in frontend `.env.local`
- Check firewall settings

---

#### 7. **Python Dependencies Missing**
```
Error: ModuleNotFoundError: No module named 'onnxruntime'
```
**Solution:**
```bash
cd backend
pip install -r requirements.txt

# Or manually:
pip install onnxruntime numpy librosa
```

---

### Debug Mode

Enable verbose logging:

```bash
# Backend
NODE_DEBUG=express npm run dev

# With Python logging
DEBUG=* npm run dev

# Frontend
next dev --debug
```

### Health Check Endpoints

```bash
# Server status
curl http://localhost:8080/api/health

# All services
curl http://localhost:8080/api/health/services

# ML models
curl http://localhost:8080/api/health/models
```

---

## üìö Additional Resources

### Documentation Files
- **Backend**: `backend/README_DETAILED.md`
- **Backend Services**: Each service has `README.md`
- **Frontend**: `frontend/ARCHITECTURE.md`
- **Copilot Instructions**: `.github/copilot-instructions.md`

### API Documentation
- [Groq API Docs](https://console.groq.com/docs)
- [Google Gemini API](https://ai.google.dev/)
- [HuggingFace Inference](https://huggingface.co/docs/api-inference)
- [Supabase Docs](https://supabase.com/docs)

### ML Model Resources
- [ONNX Runtime](https://onnxruntime.ai/)
- [Transformers Library](https://huggingface.co/docs/transformers)
- [Wav2Vec2 Model](https://huggingface.co/docs/transformers/model_doc/wav2vec2)

---

## ü§ù Contributing

### Code Standards
- Follow ESLint rules (frontend)
- Use consistent naming conventions
- Document complex logic with comments
- Test before pushing

### Pull Request Process
1. Create feature branch: `git checkout -b feature/your-feature`
2. Make changes following code standards
3. Test thoroughly
4. Commit with clear messages
5. Push and create pull request

---

## üìÑ License

This project is part of the EmotionSense AI initiative.

---

## üë§ Author & Support

**Created by**: Ayush Singh

For questions or support, refer to the project documentation or contact the development team.

---

**Last Updated**: November 2025
**Version**: 1.0.0
**Status**: Active Development
